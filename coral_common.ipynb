{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8513baff-cafb-4967-87cf-7f9f03072744",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"https://10botics.com/logo_jnb.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c86c5-37ea-4498-b558-7b575bb783ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coral common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96177708-f084-44c4-9c96-e03249515e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "This files import libraries and contains common functions needed by the squat counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1bd817-f4f2-4df4-aba8-735eb9872de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the camera functions\n",
    "\n",
    "# %run coral_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb06ef-d08f-4419-b727-249a426595a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def putText(frame, text, text_color = (0, 255, 0), text_position = (50, 50)):\n",
    "    # Define the text properties\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_scale = 0.65\n",
    "    text_thickness = 2\n",
    "\n",
    "    # Add text annotation on the frame\n",
    "    cv2.putText(frame, text, text_position, font, text_scale, text_color, text_thickness)\n",
    "\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle\n",
    "\n",
    "def calculate_hip_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "    \n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_shoulder\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_shoulder\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # if confident score of keypoints are all above threshold, return the midpoint of two angle\n",
    "    # otherwise, return None\n",
    "    if (a1s>keypoint_threshold)*(b1s>keypoint_threshold)*(c1s>keypoint_threshold):\n",
    "        return (angle1 + angle2) / 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def calculate_knee_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "\n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_ankle\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_ankle\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # if confident score of keypoints are all above threshold, return the midpoint of two angle\n",
    "    # otherwise, return None\n",
    "    if (a1s>keypoint_threshold)*(b1s>keypoint_threshold)*(c1s>keypoint_threshold):\n",
    "        return (angle1 + angle2) / 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def createStopButton():\n",
    "    return  widgets.ToggleButton(\n",
    "        value=False,\n",
    "        description='Stop',\n",
    "        disabled=False,\n",
    "        button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Description',\n",
    "        icon='square' # (FontAwesome names without the `fa-` prefix)\n",
    "    )\n",
    "\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Display function\n",
    "# ================\n",
    "import time\n",
    "\n",
    "def view( button, source=0, rotate=0, process=(lambda x:x)):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    button: An IPywidget button to control the function. The display stops if button.value == True.\n",
    "    source: An optional integer or filename to specify the source of the video stream.\n",
    "    rotate: optional integer. Set to 0 by default (not rotated). Set to 1 will rotate by 90 deg.\n",
    "    process: optional function that processes each frame through process(frame) before displaying.\n",
    "        Set to identity function by default.\n",
    "    \"\"\"\n",
    "    global time_process, start_time\n",
    "    time_process = []\n",
    "    \n",
    "    display_handle=display(\"Please wait...\", display_id=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(source)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        start_time_process = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if button.value==True or not ret:\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n",
    "            button.value = False\n",
    "            break\n",
    "\n",
    "        #frame = cv2.flip(frame, 1)  # if the camera mirrors your image\n",
    "\n",
    "        # Rotate the frame if rotate==1\n",
    "        if rotate:\n",
    "            # frame = cv2.transpose(frame)\n",
    "            frame = cv2.flip(frame, 0)\n",
    "        \n",
    "        frame = process(frame)\n",
    "        \n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(IPython.display.Image(data=frame.tobytes()))\n",
    "\n",
    "        time_process.append(time.time() - start_time_process)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48487d74-ad59-427c-8477-cebfd109f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from pycoral.adapters import common\n",
    "from pycoral.utils.edgetpu import make_interpreter\n",
    "\n",
    "interpreter = make_interpreter(\"movenet_single_pose_lightning_ptq_edgetpu.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "_NUM_KEYPOINTS = 17\n",
    "\n",
    "def get_result_from_image_with_model(input, interpreter, output=None):\n",
    "    '''\n",
    "    This function takes an input image and applies a pre-trained pose estimation model \n",
    "    to extract the position of human body keypoints in the image. \n",
    "    The TensorFlow Lite interpreter object is used to run inference on the input image\n",
    "    using the pre-trained model.\n",
    "    If the optional output argument is provided, the function also saves the input image\n",
    "    with the detected keypoints drawn on it to the specified file path.\n",
    "\n",
    "    Args:\n",
    "    input: An input image file path or a numpy array representing an image.\n",
    "    interpreter: A TensorFlow Lite interpreter object.\n",
    "    output (optional): A file path to save the output image with keypoints drawn on it.\n",
    "\n",
    "    Return:\n",
    "    Array of keypoints. Each keypoint is represented as a triplet of [y, x, score].\n",
    "    '''\n",
    "    # load the input image using PIL\n",
    "    if isinstance(input, str):\n",
    "        img = PIL.Image.open(input)\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        img = PIL.Image.fromarray(cv2.cvtColor(input, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # resize the image to model required size\n",
    "    resized_img = img.resize(common.input_size(interpreter), PIL.Image.Resampling.LANCZOS)\n",
    "\n",
    "    # load the resized image to interpreter\n",
    "    common.set_input(interpreter, resized_img)\n",
    "\n",
    "    # conduct the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # reshape and assign the inference result to variable `pose`\n",
    "    keypoints_with_scores = common.output_tensor(interpreter, 0).copy().reshape(_NUM_KEYPOINTS, 3)\n",
    "\n",
    "    # draw the keypoints and save the image (if specified `output`)\n",
    "    if output:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        width, height = img.size\n",
    "        for i in range(0, _NUM_KEYPOINTS):\n",
    "            draw.ellipse(\n",
    "            xy=[\n",
    "                keypoints_with_scores[i][1] * width - 2, keypoints_with_scores[i][0] * height - 2,\n",
    "                keypoints_with_scores[i][1] * width + 2, keypoints_with_scores[i][0] * height + 2\n",
    "            ],\n",
    "            fill=(255, 0, 0))\n",
    "        img.save(output)\n",
    "        \n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cccca4b-bbac-4aa2-9743-ad73c8337fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b376544-2517-4b95-913f-ce6f522ce5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Congratulation! You have finished this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a5467-8b99-4112-a85f-8c63a83d575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "This jupyter notebook is created by 10Botics. <br>\n",
    "For permission to use in school, please contact info@10botics.com <br>\n",
    "All rights reserved. 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
