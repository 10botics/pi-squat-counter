{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efbc1fe-bc44-45b8-bcc9-f0641ffb08c2",
   "metadata": {},
   "source": [
    "# Posture detection using tensorflow判斷人體姿態<br>**Bonus**: Step 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563244e-c531-4cc1-bcca-bb2374dcdca8",
   "metadata": {},
   "source": [
    "##### We have successfully implemented the Squat Counting function in Step 6 just now. <br>It looks good, but are there still some defects? <br>剛剛我們在Step6中，成功運行到Squat Counting的功能。<br>這樣看起來很不錯，但是是否仍然有一些缺陷？<br><br>Let's evaluate our Step 6 code again in terms of speed:<br>讓我們在速度上再次評估我們Step6的代碼: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b7ee65-7368-4413-a16b-74379a16655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Importing from step 3 ... ======\n",
      "Libraries imported.\n",
      "Model loaded.\n",
      "Image loaded.\n",
      "\n",
      "The following functions have been imported:\n",
      "   movenet(input_image)\n",
      "   _keypoints_and_edges_for_display(keypoints_with_scores, height, width, keypoint_threshold=0.11)\n",
      "   draw_prediction_on_image(image, keypoints_with_scores, crop_region=None, close_figure=False, output_image_height=None)\n",
      "The following global variables have been assigned:\n",
      "   module, model_name, input_size, KEYPOINT_DICT, KEYPOINT_EDGE_INDS_TO_COLOR\n",
      "=======================================\n",
      "\n",
      "====== Importing from step 4 ... ======\n",
      "\n",
      "The following functions have been imported:\n",
      "   load_image_to_tf(image)\n",
      "   tf_image_to_model(image, width, height)\n",
      "   get_keypoints_with_scores_from_image_with_movenet(image, width, height)\n",
      "   show_image(image)\n",
      "=======================================\n",
      "    \n",
      "====== Importing from step 5 ... ======\n",
      "\n",
      "The following functions have been imported:\n",
      "   get_keypoints_and_edges_from_image_with_movenet(image, width, height)\n",
      "   draw_output_from_image_with_movenet(image)\n",
      "   view( button, source=0, rotate=0, process=(lambda x:x) )\n",
      "=======================================\n",
      "    \n",
      "====== Importing from step 6 ... ======\n"
     ]
    }
   ],
   "source": [
    "from step6 import *\n",
    "from step6 import _keypoints_and_edges_for_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8b8896-2c87-4c0c-ad3c-987d3ddec781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c57d11f5e924c49b020d341df0fee11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "time_elapsed = []\n",
    "\n",
    "def frame_process(frame):\n",
    "    global repetition, squat, squatting\n",
    "    # calculate and display angle    \n",
    "    \n",
    "    ''' mark the start time of MoveNet Conversion'''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    keypoints_with_scores = get_keypoints_with_scores_from_image_with_movenet(frame,input_size,input_size)\n",
    "    \n",
    "    ''' mark the end time of MoveNet Conversion'''\n",
    "    ''' endtime - start_time is the time elapsed for MoveNet to process a frame'''\n",
    "    time_elapsed.append( time.time() - start_time )\n",
    "    \n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n",
    "    # check if confidence is qualified, if yes, proceed to judgement\n",
    "    if hip_angle is not None and knee_angle is not None:\n",
    "        # if capture continuous CONSECUTIVE_TH angles in squatting, change squat to 1\n",
    "        # if capture continuous CONSECUTIVE_TH angles in standing, change squat to 0 and repetition += 1\n",
    "        if hip_angle<125 and knee_angle<105:\n",
    "            if squat == 0:\n",
    "                squatting += 1\n",
    "            else:\n",
    "                squatting = CONSECUTIVE_TH\n",
    "        elif hip_angle>145 and knee_angle>125:\n",
    "            if squat == 1:\n",
    "                squatting -= 1\n",
    "            else:\n",
    "                squatting = 0\n",
    "        if squatting == 0 and squat == 1:\n",
    "            repetition += 1\n",
    "            squat = 0\n",
    "        elif squatting == CONSECUTIVE_TH and squat == 0:\n",
    "            squat = 1\n",
    "        text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    else:\n",
    "        text = f\"Reps: {repetition}  Knee: ?  Hip: ?\"\n",
    "    if squat:\n",
    "        text_color = (0,255,0)\n",
    "    else:\n",
    "        text_color = (128,192,64)\n",
    "    putText(frame, text, text_color)\n",
    "    return frame\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display(stopButton)\n",
    "    thread = threading.Thread(target=view, args=(stopButton, \"Produce_2.mp4\", 0, frame_process))\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097f28ae-c732-4b3b-a577-93ee58c9882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average running time of Movenet is: 0.12882413014326946\n",
      "Average frame rate of Movenet is: 7.762520879340445\n"
     ]
    }
   ],
   "source": [
    "average_time = sum(time_elapsed) / len(time_elapsed)\n",
    "\n",
    "print(f\"Average running time of Movenet is: { average_time }\")\n",
    "print(f\"Average frame rate of Movenet is: { 1 / average_time }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11decd-28fc-48c4-924a-5e924bd11f06",
   "metadata": {},
   "source": [
    "##### <br><hr> After completing the test, please restart the IPython Kernel and do not run any of the cells above again.<br>完成測試後，請重啓IPython Kernel，並**不要再運行**上面的任何單元格\n",
    "<br>![img7-1.png](./resource/img7-1.png)\n",
    "<br><hr>\n",
    "Although our Step 6 code can achieve the basic function of squat counting, the slow inference of Movenet causes our counter to run less smoothly and responsively. <br>\n",
    "<br>\n",
    "Movenet is a complex deep learning model that requires a large amount of computational resources. The CPU and memory of Raspberry Pi 4 are limited, and cannot provide enough resources to run Movenet in real-time.\n",
    "可以看出，雖然我們Step6的代碼能夠實現深蹲計數的基本功能，但由於Movenet的推理太過緩慢，使得我們的計數器運行不夠流暢和靈敏。<br>\n",
    "Movenet是一種複雜的深度學習模型，需要大量的計算資源，而Raspberry Pi 4的處理器和內存容量有限，無法提供足夠的計算資源來實時運行Movenet。<br>\n",
    "<br>\n",
    "The Coral accelerator offers a solution for this problem. It's designed to speed up deep learning inference and can be used with Raspberry Pi 4. It boosts the model inference speed, enabling smoother and real-time operation of Movenet on Raspberry Pi 4.<br>\n",
    "而針對這個問題，Coral accelerator可以提供一個解決方案。Coral accelerator是一種專門設計用於加速深度學習推理的硬體加速器，可以與Raspberry Pi 4一起使用。Coral accelerator可以大大提高模型推理速度，從而使得在Raspberry Pi 4上運行Movenet變得更加流暢和實時。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42c317-1184-4311-945b-655376b9ffdf",
   "metadata": {},
   "source": [
    "##### <hr>First, we import the necessary libraries and redefine some of the functions used before.<br>首先，我們導入所需要的庫，並重新定義一些之前的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c539ab8d-3a41-4c29-9283-0fe78fff0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def putText(frame, text, color = (0, 255, 0)):\n",
    "    # Define the text properties\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_position = (50, 50)\n",
    "    text_scale = 0.65\n",
    "    text_color = color\n",
    "    text_thickness = 2\n",
    "\n",
    "    # Add text annotation on the frame\n",
    "    cv2.putText(frame, text, text_position, font, text_scale, text_color, text_thickness)\n",
    "\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle\n",
    "\n",
    "def calculate_hip_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "    \n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_shoulder\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_shoulder\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # if confident score of keypoints are all above threshold, return the midpoint of two angle\n",
    "    # otherwise, return None\n",
    "    if (a1s>keypoint_threshold)*(b1s>keypoint_threshold)*(c1s>keypoint_threshold):\n",
    "        return (angle1 + angle2) / 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def calculate_knee_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "\n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_ankle\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_ankle\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # if confident score of keypoints are all above threshold, return the midpoint of two angle\n",
    "    # otherwise, return None\n",
    "    if (a1s>keypoint_threshold)*(b1s>keypoint_threshold)*(c1s>keypoint_threshold):\n",
    "        return (angle1 + angle2) / 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Description',\n",
    "    icon='square' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Display function\n",
    "# ================\n",
    "def view( button, source=0, rotate=0, process=(lambda x:x) ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    button: An IPywidget button to control the function. The display stops if button.value == True.\n",
    "    Source: An optional integer or filename to specify the source of the video stream.\n",
    "    Rotate: optional integer. Set to 0 by default (not rotated). Set to 1 will rotate by 90 deg.\n",
    "    process: optional function that processes each frame through process(frame) before displaying.\n",
    "        Set to identity function by default.\n",
    "    \"\"\"\n",
    "    display_handle=display(\"Please wait...\", display_id=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(source)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if button.value==True or not ret:\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n",
    "            button.value = False\n",
    "            break\n",
    "        # frame = cv2.flip(frame, 1) # if your camera reverses your image\n",
    "\n",
    "        # Rotate the frame if rotate==1\n",
    "        if rotate:\n",
    "            frame = cv2.transpose(frame)\n",
    "            frame = cv2.flip(frame, 0)\n",
    "        \n",
    "        frame = process(frame)\n",
    "        \n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(IPython.display.Image(data=frame.tobytes()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a837fe6-b9d8-4788-b72a-ab24acd19ee8",
   "metadata": {},
   "source": [
    "##### Next, let's try using Python to drive the Coral Accelerator to perform image inference.<br>接下來，讓我們試着用python驅動Coral Accelerator去執行圖片推理<br>\n",
    "After importing the relevant libraries, we define the `get_result_from_image_with_model`. This function takes an image as input and applies a pre-trained pose estimation model to extract the positions of human keypoints in the image.<br>\n",
    "導入相關庫之後，我們定義了`get_result_from_image_with_model`函數。這個函數需要輸入一張圖像並應用預先訓練的姿勢判斷模型，以提取圖像中人體關鍵點的位置。<br>\n",
    "With this function, we can use the Coral Accelerator to quickly obtain the `keypoints_with_scores` result by providing an image and an interpreter. Moreover, the code is much shorter than before!<br>\n",
    "這樣一來，我們就可以利用這個函數，給定一張圖像和解釋器，就能夠利用Coral Accelerator快速得出keypoints_with_scores的結果了！而且比之前我們的代碼簡短了許多！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197d7816-3ee2-4415-bc64-f77f1abbebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "\n",
      "(17, 3)\n",
      "\n",
      "[[0.32774833 0.58175325 0.36462   ]\n",
      " [0.3113609  0.589947   0.70056206]\n",
      " [0.3113609  0.5694627  0.5694627 ]\n",
      " [0.31955463 0.5735596  0.29907033]\n",
      " [0.3113609  0.50391304 0.49981618]\n",
      " [0.42197597 0.5571721  0.49981618]\n",
      " [0.40558854 0.43016967 0.49981618]\n",
      " [0.5121068  0.70875573 0.29907033]\n",
      " [0.5080099  0.53668785 0.8029834 ]\n",
      " [0.42197597 0.6841746  0.5694627 ]\n",
      " [0.4096854  0.65549666 0.19664899]\n",
      " [0.6022375  0.38100743 0.70056206]\n",
      " [0.6022375  0.28268293 0.70056206]\n",
      " [0.6473029  0.58175325 0.43016967]\n",
      " [0.6841746  0.41378227 0.43016967]\n",
      " [0.83166134 0.5162036  0.24581124]\n",
      " [0.88901734 0.33184516 0.75382113]]\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from pycoral.adapters import common\n",
    "from pycoral.utils.edgetpu import make_interpreter\n",
    "\n",
    "interpreter = make_interpreter(\"movenet_single_pose_lightning_ptq_edgetpu.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "_NUM_KEYPOINTS = 17\n",
    "\n",
    "def get_result_from_image_with_model(input, interpreter, output=None):\n",
    "    '''\n",
    "    This function takes an input image and applies a pre-trained pose estimation model \n",
    "    to extract the position of human body keypoints in the image. \n",
    "    The TensorFlow Lite interpreter object is used to run inference on the input image\n",
    "    using the pre-trained model.\n",
    "    If the optional output argument is provided, the function also saves the input image\n",
    "    with the detected keypoints drawn on it to the specified file path.\n",
    "\n",
    "    Args:\n",
    "    input: An input image file path or a numpy array representing an image.\n",
    "    interpreter: A TensorFlow Lite interpreter object.\n",
    "    output (optional): A file path to save the output image with keypoints drawn on it.\n",
    "\n",
    "    Return:\n",
    "    Array of keypoints. Each keypoint is represented as a triplet of [y, x, score].\n",
    "    '''\n",
    "    # load the input image using PIL\n",
    "    if isinstance(input, str):\n",
    "        img = PIL.Image.open(input)\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        img = PIL.Image.fromarray(cv2.cvtColor(input, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # resize the image to model required size\n",
    "    resized_img = img.resize(common.input_size(interpreter), PIL.Image.Resampling.LANCZOS)\n",
    "\n",
    "    # load the resized image to interpreter\n",
    "    common.set_input(interpreter, resized_img)\n",
    "\n",
    "    # conduct the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # reshape and assign the inference result to variable `pose`\n",
    "    keypoints_with_scores = common.output_tensor(interpreter, 0).copy().reshape(_NUM_KEYPOINTS, 3)\n",
    "\n",
    "    # draw the keypoints and save the image (if specified `output`)\n",
    "    if output:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        width, height = img.size\n",
    "        for i in range(0, _NUM_KEYPOINTS):\n",
    "            draw.ellipse(\n",
    "            xy=[\n",
    "                keypoints_with_scores[i][1] * width - 2, keypoints_with_scores[i][0] * height - 2,\n",
    "                keypoints_with_scores[i][1] * width + 2, keypoints_with_scores[i][0] * height + 2\n",
    "            ],\n",
    "            fill=(255, 0, 0))\n",
    "        img.save(output)\n",
    "        \n",
    "    return keypoints_with_scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keypoints_with_scores = get_result_from_image_with_model(\"input_image.jpeg\", interpreter)\n",
    "    print(type(keypoints_with_scores), keypoints_with_scores.shape, keypoints_with_scores, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339c05c-ae30-45fc-852b-7aef0aaaf42d",
   "metadata": {},
   "source": [
    "##### Try it out: fill in the blank below with a Python statement that runs the hardware accelerated model, and then run the code.<br>試一試：在下面的空缺處填上一個運行硬件加速模型的python語句，然後運行代碼。<br>\n",
    "Can you run the Step 6 code and see if it works? Check the speed in the last cell.<br>\n",
    "你能運行出Step6的效果嗎？在最後的單元格看看速度如何？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536d58df-9f29-4005-b90e-05bce5e45307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046a21c61f694ec69f9ef5a52f5440c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "time_elapsed = []\n",
    "\n",
    "\n",
    "repetition = squat = squatting = 0\n",
    "CONSECUTIVE_TH=3\n",
    "def frame_process(frame):\n",
    "    global repetition, squat, squatting\n",
    "    \n",
    "    # calculate and display angle\n",
    "    start_time = time.time()\n",
    "    #   ====== ⇩ ⇩ ⇩  write your answer here   ⇩ ⇩ ⇩ ======\n",
    "    \n",
    "    keypoints_with_scores = get_result_from_image_with_model(frame, interpreter)\n",
    "    #  ====== ⇧ ⇧ ⇧  write your answer here  ⇧ ⇧ ⇧ ======\n",
    "    time_elapsed.append( time.time() - start_time )\n",
    "    \n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n",
    "\n",
    "    # check if confidence is qualified, if yes, proceed to judgement\n",
    "    if hip_angle is not None and knee_angle is not None:\n",
    "        # if capture continuous CONSECUTIVE_TH angles in squatting, change squat to 1\n",
    "        # if capture continuous CONSECUTIVE_TH angles in standing, change squat to 0 and repetition += 1\n",
    "        if hip_angle<125 and knee_angle<105:\n",
    "            if squat == 0:\n",
    "                squatting += 1\n",
    "            else:\n",
    "                squatting = CONSECUTIVE_TH\n",
    "        elif hip_angle>145 and knee_angle>125:\n",
    "            if squat == 1:\n",
    "                squatting -= 1\n",
    "            else:\n",
    "                squatting = 0\n",
    "        if squatting == 0 and squat == 1:\n",
    "            repetition += 1\n",
    "            squat = 0\n",
    "        elif squatting == CONSECUTIVE_TH and squat == 0:\n",
    "            squat = 1\n",
    "        text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    else:\n",
    "        text = f\"Reps: {repetition}  Knee: ?  Hip: ?\"\n",
    "    if squat:\n",
    "        text_color = (0,255,0)\n",
    "    else:\n",
    "        text_color = (128,192,64)\n",
    "        \n",
    "    putText(frame, text, text_color)\n",
    "    return frame\n",
    "\n",
    "\n",
    "# Run\n",
    "# ================\n",
    "if __name__ == \"__main__\":\n",
    "    display(stopButton)\n",
    "    thread = threading.Thread(target=view, args=(stopButton, \"Produce_2_direct.mp4\", 0, frame_process))\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49efd84-6e59-497c-9b59-d9e991d84cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_time = sum(time_elapsed) / len(time_elapsed)\n",
    "\n",
    "print(f\"Average running time of Movenet is: { average_time }\")\n",
    "print(f\"Average frame rate of Movenet is: { 1 / average_time }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
