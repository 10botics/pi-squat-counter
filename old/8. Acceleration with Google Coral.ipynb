{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efbc1fe-bc44-45b8-bcc9-f0641ffb08c2",
   "metadata": {},
   "source": [
    "# Acceleration with Google Coral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11decd-28fc-48c4-924a-5e924bd11f06",
   "metadata": {},
   "source": [
    "\n",
    "Although our code for Step 6 can achieve the basic function of counting squats, due to the heavy computation resources needed by Movenet, the operation is too slow, resulting in our counter running less smoothly and responsively.<br>\n",
    "雖然我們Step6的代碼能夠實現深蹲計數的基本功能，但由於Movenet需要大量的計算資源，運行太過緩慢，使得我們的計數器運行不夠流暢和靈敏。<br>\n",
    "<br>\n",
    "The Coral accelerator is a hardware accelerator specifically designed to speed up deep learning inference, and can be used with Raspberry Pi 4. It can greatly improve the model inference speed, making it more smooth to run Movenet on Raspberry Pi 4.<br>\n",
    "Coral accelerator是一種專門設計用於加速深度學習推理的硬體加速器，可以與Raspberry Pi 4一起使用。Coral accelerator可以大大提高模型推理速度，從而使得在Raspberry Pi 4上運行Movenet變得更加流暢。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad09115e-77a5-4ae9-8de9-e2cbc5f05465",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run coral_common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a837fe6-b9d8-4788-b72a-ab24acd19ee8",
   "metadata": {},
   "source": [
    "##### Next, let's try using Python to drive the Coral Accelerator to perform image inference.<br>接下來，讓我們試着用python驅動Coral Accelerator去執行圖片推理<br>\n",
    "After importing the libraries, we defined function `get_result_from_image_with_model`. This function takes an image as input and applies a pre-trained pose detection model to extract the positions of key points on the human body in the image. With this function, we can quickly obtain MoveNet results using the Coral Accelerator by simply providing an image and an interpreter.<br>\n",
    "導入相關庫之後，我們定義了`get_result_from_image_with_model`函數。這個函數需要輸入一張圖像並應用預先訓練的姿勢判斷模型，以提取圖像中人體關鍵點的位置。這個函數讓我們只需給定一張圖像和解釋器，就能夠利用Coral Accelerator快速得出MoveNet結果了。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197d7816-3ee2-4415-bc64-f77f1abbebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "\n",
      "(17, 3)\n",
      "\n",
      "[[0.32774833 0.58175325 0.36462   ]\n",
      " [0.3113609  0.589947   0.70056206]\n",
      " [0.3113609  0.5694627  0.5694627 ]\n",
      " [0.31955463 0.5735596  0.29907033]\n",
      " [0.3113609  0.50391304 0.49981618]\n",
      " [0.42197597 0.5571721  0.49981618]\n",
      " [0.40558854 0.43016967 0.49981618]\n",
      " [0.5121068  0.70875573 0.29907033]\n",
      " [0.5080099  0.53668785 0.8029834 ]\n",
      " [0.42197597 0.6841746  0.5694627 ]\n",
      " [0.4096854  0.65549666 0.19664899]\n",
      " [0.6022375  0.38100743 0.70056206]\n",
      " [0.6022375  0.28268293 0.70056206]\n",
      " [0.6473029  0.58175325 0.43016967]\n",
      " [0.6841746  0.41378227 0.43016967]\n",
      " [0.83166134 0.5162036  0.24581124]\n",
      " [0.88901734 0.33184516 0.75382113]]\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from pycoral.adapters import common\n",
    "from pycoral.utils.edgetpu import make_interpreter\n",
    "\n",
    "interpreter = make_interpreter(\"movenet_single_pose_lightning_ptq_edgetpu.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "_NUM_KEYPOINTS = 17\n",
    "\n",
    "def get_result_from_image_with_model(input, interpreter, output=None):\n",
    "    '''\n",
    "    This function takes an input image and applies a pre-trained pose estimation model \n",
    "    to extract the position of human body keypoints in the image. \n",
    "    The TensorFlow Lite interpreter object is used to run inference on the input image\n",
    "    using the pre-trained model.\n",
    "    If the optional output argument is provided, the function also saves the input image\n",
    "    with the detected keypoints drawn on it to the specified file path.\n",
    "\n",
    "    Args:\n",
    "    input: An input image file path or a numpy array representing an image.\n",
    "    interpreter: A TensorFlow Lite interpreter object.\n",
    "    output (optional): A file path to save the output image with keypoints drawn on it.\n",
    "\n",
    "    Return:\n",
    "    Array of keypoints. Each keypoint is represented as a triplet of [y, x, score].\n",
    "    '''\n",
    "    # load the input image using PIL\n",
    "    if isinstance(input, str):\n",
    "        img = PIL.Image.open(input)\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        img = PIL.Image.fromarray(cv2.cvtColor(input, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # resize the image to model required size\n",
    "    resized_img = img.resize(common.input_size(interpreter), PIL.Image.Resampling.LANCZOS)\n",
    "\n",
    "    # load the resized image to interpreter\n",
    "    common.set_input(interpreter, resized_img)\n",
    "\n",
    "    # conduct the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # reshape and assign the inference result to variable `pose`\n",
    "    keypoints_with_scores = common.output_tensor(interpreter, 0).copy().reshape(_NUM_KEYPOINTS, 3)\n",
    "\n",
    "    # draw the keypoints and save the image (if specified `output`)\n",
    "    if output:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        width, height = img.size\n",
    "        for i in range(0, _NUM_KEYPOINTS):\n",
    "            draw.ellipse(\n",
    "            xy=[\n",
    "                keypoints_with_scores[i][1] * width - 2, keypoints_with_scores[i][0] * height - 2,\n",
    "                keypoints_with_scores[i][1] * width + 2, keypoints_with_scores[i][0] * height + 2\n",
    "            ],\n",
    "            fill=(255, 0, 0))\n",
    "        img.save(output)\n",
    "        \n",
    "    return keypoints_with_scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keypoints_with_scores = get_result_from_image_with_model(\"input_image.jpeg\", interpreter)\n",
    "    print(type(keypoints_with_scores), keypoints_with_scores.shape, keypoints_with_scores, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339c05c-ae30-45fc-852b-7aef0aaaf42d",
   "metadata": {},
   "source": [
    "Can this code produce the effect of Step6? Check the speed in the last cell.<br>\n",
    "這個代碼能運行出Step6的效果嗎？在最後的單元格看看速度如何？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b160b69-c87d-4ee8-b3a5-4861e32bb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_elapsed = []\n",
    "\n",
    "repetition = squat = squatting = 0\n",
    "CONSECUTIVE_TH=3\n",
    "def frame_process(frame):\n",
    "    global repetition, squat, squatting\n",
    "    \n",
    "    # calculate and display angle\n",
    "    start_time = time.time()\n",
    "    #   ====== ⇩ ⇩ ⇩  write your answer here   ⇩ ⇩ ⇩ ======\n",
    "    \n",
    "    keypoints_with_scores = get_result_from_image_with_model(frame, interpreter)\n",
    "    #  ====== ⇧ ⇧ ⇧  write your answer here  ⇧ ⇧ ⇧ ======\n",
    "    time_elapsed.append( time.time() - start_time )\n",
    "    \n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n",
    "\n",
    "    # check if confidence is qualified, if yes, proceed to judgement\n",
    "    if hip_angle is not None and knee_angle is not None:\n",
    "        # if capture continuous CONSECUTIVE_TH angles in squatting, change squat to 1\n",
    "        # if capture continuous CONSECUTIVE_TH angles in standing, change squat to 0 and repetition += 1\n",
    "        if hip_angle<125 and knee_angle<105:\n",
    "            if squat == 0:\n",
    "                squatting += 1\n",
    "            else:\n",
    "                squatting = CONSECUTIVE_TH\n",
    "        elif hip_angle>145 and knee_angle>125:\n",
    "            if squat == 1:\n",
    "                squatting -= 1\n",
    "            else:\n",
    "                squatting = 0\n",
    "        if squatting == 0 and squat == 1:\n",
    "            repetition += 1\n",
    "            squat = 0\n",
    "        elif squatting == CONSECUTIVE_TH and squat == 0:\n",
    "            squat = 1\n",
    "        text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    else:\n",
    "        text = f\"Reps: {repetition}  Knee: ?  Hip: ?\"\n",
    "    if squat:\n",
    "        text_color = (0,255,0)\n",
    "    else:\n",
    "        text_color = (128,192,64)\n",
    "        \n",
    "    putText(frame, text, text_color)\n",
    "    return frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1be1f-4aed-4395-8e4e-6c24f9829863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1267dbc2-b3aa-4e20-8f5c-93ade12bda88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'createStopButton' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stopButton \u001b[38;5;241m=\u001b[39m \u001b[43mcreateStopButton\u001b[49m()\n\u001b[1;32m      2\u001b[0m display(stopButton)\n\u001b[1;32m      3\u001b[0m thread \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mview, args\u001b[38;5;241m=\u001b[39m(stopButton, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduce_2_direct.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, frame_process))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'createStopButton' is not defined"
     ]
    }
   ],
   "source": [
    "stopButton = createStopButton()\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton, \"Produce_2_direct.mp4\", 0, frame_process))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8aa6d2-a9e8-4c1a-b2ab-fd2ddac5e2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef661832339c4782a49a17736b02debc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=True, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Run\n",
    "# ================\n",
    "if __name__ == \"__main__\":\n",
    "    display(stopButton)    \n",
    "    thread = threading.Thread(target=view, args=(stopButton, 0, 1, frame_process))\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49efd84-6e59-497c-9b59-d9e991d84cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_time = sum(time_process) / len(time_process)\n",
    "\n",
    "print(f\"Average process time per frame is: { average_time }\")\n",
    "print(f\"Average frame rate is: { 1 / average_time }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
