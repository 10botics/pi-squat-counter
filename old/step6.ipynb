{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d9a3941-0e48-4dcb-93fd-f28598f63278",
   "metadata": {},
   "source": [
    "# Posture detection using tensorflow判斷人體姿態<br>Step 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae15aabb-9970-4c26-9d57-f9c129c66e9a",
   "metadata": {},
   "source": [
    "### 3. Squat detection and counting 深蹲判斷與計數"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ab830b3-651b-4b74-8db9-28ce63715b95",
   "metadata": {},
   "source": [
    "In squat exercises, though **knee and hip joint angles** vary by individual ability, flexibility, and squatting techniques, these can be used as effective features to determine squatting down or standing up.<br>深蹲運動中，雖然膝蓋與臀部關節角度可能因個人體能、柔軟度和深蹲技巧而有所不同，但一般來說，**膝蓋與臀部關節的角度**可以作爲我們判別蹲下或起立的有效特徵。<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "148aa0a7-a716-4c5a-9900-f006897c4313",
   "metadata": {},
   "source": [
    "Above, we performed pose estimation on an image using MoveNet and defineed `get_keypoints_with_scores_from_image_with_movenet` to directly obtain MoveNet results from an image.<br>上面，我們可以通過MoveNet對圖像進行姿態辨識，並定義了`get_keypoints_with_scores_from_image_with_movenet`來直接從一張圖片獲取MoveNet結果<br>\n",
    "<br>\n",
    "How can we convert these results into angle?<br>\n",
    "我們如何把這些結果轉換成角度信息呢？<br>\n",
    "![img6-1](resource/img6-1.png)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c5a63e7-a24f-43ce-9032-d3b10643a65c",
   "metadata": {},
   "source": [
    "##### The `calculate_angle(a, b, c)` in the cell below obtains angle information using a such method.<br>下方單元格的`calculate_angle(a,b,c)`函數就是運用類似這樣的方法獲取角度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d2ff2e-54a4-4c10-b4fd-5abc15db9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f59f42-76d2-45e1-b3b5-c18acf1dbc4e",
   "metadata": {},
   "source": [
    "##### Let's use the `calculate_angle(a,b,c)` previously defined to create a function that calculates the angle between the knee and hip joints.<br>讓我們利用上方定義的calculate_angle(a,b,c)封裝計算膝關節與臀部角度的函數\n",
    "The input of the function is the keypoints detected by the MoveNet model with scores.<br>\n",
    "函數的輸入是MoveNet模型檢測到的帶有分數的關鍵點。<br>\n",
    "<br>\n",
    "`calculate_hip_joint_angle` calculates the angle of the left and right hip joints. It extracts positions of the hips, knees, and shoulders from keypoints, and calculates angles from the position. It finally returns the average of these two angles.<br>\n",
    "`calculate_hip_joint_angle`計算左右臀部的角度。它首先從關鍵點中提取左右臀部，膝蓋和肩膀的位置，然後從位置信息計算角度。最後返回這兩個角度的平均值。<br>\n",
    "<br>\n",
    "`calculate_hip_joint_angle` calculates the angle of the left and right knee joints in the same way<br>\n",
    "`calculate_hip_joint_angle` 函數用同樣的方法計算左右膝關節的角度。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b96f9a-03c5-4e55-a650-5347f199cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hip_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "    keypoints_with_scores = keypoints_with_scores[0][0]\n",
    "    \n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_shoulder\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_shoulder\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # return the midpoint of two angle\n",
    "    return (angle1 + angle2) / 2\n",
    "    \n",
    "\n",
    "def calculate_knee_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "    keypoints_with_scores = keypoints_with_scores[0][0]\n",
    "\n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_ankle\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_ankle\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # return the midpoint of two angle\n",
    "    return (angle1 + angle2) / 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca12a1d5-3aee-4420-9fa6-1dd42ea48338",
   "metadata": {},
   "source": [
    "##### Next, we will use OpenCV and functions above to detect body posture and calculate the number of squats.<br>接下來，我們利用OpenCV和上面的函數，檢測人的身體姿勢計算深蹲次數。\n",
    "<br>\n",
    "In each iteration, we pass the image frame to MoveNet and obtain the results, and then convert the results of MoveNet into angles of the knee joint and hip.<br>\n",
    "在每次循環中，我們將圖像幀傳給MoveNet並獲取姿態檢測的結果，將MoveNet的結果轉換爲膝關節和臀部的角度。<br>\n",
    "<br>\n",
    "When the angle is below a certain value, the `squat` is recorded as 1 (indicates squatted down). Otherwise, otherwise the `squat` is recorded as 0 (indicates standed up). We add 1 to the squat count when transitioning from squat to stand. (i.e., when squat==1 and angle is greater than the threshold ).<br>\n",
    "當角度小於一定值時，將`squat`記爲1（表示已蹲下）。否則，將`squat`記爲0（表示已起立）。我們將姿態從蹲下轉爲起立的瞬間（squat==1的同時角度大於判定值）將深蹲計數+1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "166740f8-4d98-4887-8c95-c4f1bfbb7a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70213ebc578f4e1683d838162dff16d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def putText(frame, text, color = (0, 255, 0)):\n",
    "    # Define the text properties\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_position = (50, 50)\n",
    "    text_scale = 0.65\n",
    "    text_color = color\n",
    "    text_thickness = 2\n",
    "\n",
    "    # Add text annotation on the frame\n",
    "    cv2.putText(frame, text, text_position, font, text_scale, text_color, text_thickness)\n",
    "\n",
    "repetition = 0\n",
    "squat = 0\n",
    "def frame_process(frame):\n",
    "    global repetition, squat\n",
    "    # calculate and display angle\n",
    "    keypoints_with_scores = get_keypoints_with_scores_from_image_with_movenet(frame,input_size,input_size)\n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n",
    "\n",
    "\n",
    "    # Gesture judgement from angle of hip-joint and knee-joint\n",
    "    # if gesture is from squat (squat == 1) change to stand (squat == 0), repetition add by 1\n",
    "    if hip_angle<135 and knee_angle<120:\n",
    "        squat = 1\n",
    "    elif hip_angle>135 and knee_angle>120:\n",
    "        if squat == 1:\n",
    "            repetition += 1\n",
    "        squat = 0\n",
    "    text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    if squat:\n",
    "        text_color = (0,255,0)\n",
    "    else:\n",
    "        text_color = (128, 192, 64)\n",
    "\n",
    "    # draw visualized prediction from MoveNet on the frame. Attention: Not recommended, VERY LAGGY!\n",
    "    #frame = draw_prediction_on_image( tf_image_to_model(frame,600,600)[0]/255, keypoints_with_scores)\n",
    "    \n",
    "    \n",
    "    putText(frame, text, text_color)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "# Run\n",
    "# ================\n",
    "if __name__ == \"__main__\":\n",
    "    display(stopButton)\n",
    "    thread = threading.Thread(target=view, args=(stopButton, \"Produce_2.mp4\", 0, frame_process))\n",
    "    thread.start()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3646147f-e71a-4b2b-bc96-3fdb8c44f587",
   "metadata": {},
   "source": [
    "##### <br><hr>Evidently, the output results of MoveNet are mostly reliable, but sometimes there may be some jitter, which can cause counting errors! <br>很明顯，MoveNet的輸出結果雖然大多是可靠的，但有時會有所抖動，而這些抖動會造成錯誤計數！<br>\n",
    "What are the possible solutions?<br>\n",
    "有哪些解決方案呢？<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f593d4ff-ebb3-4f44-b76c-cbe542ff1731",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1. Output prone to shaking in **half-squat position**?<br>**半蹲狀態**的輸出容易抖動？<br><br>\n",
    "Separate the detection threshold for standing and squatting by a certain distance. Specifically, we set two detection thresholds separated by a certain distance. <br>\n",
    "Only when the angle is lower than the lower threshold, the status is set as \"squatting\"; only when the angle is higher than the higher threshold, the status is set as \"standing\". <br>\n",
    "This design can effectively filter out the result fluctuations caused by small-range fluctuations in the measurement value during the binarization process.<br>\n",
    "將站立和蹲下的判斷閾值分開一定的距離。具體而言，我們設置兩個判斷閾值，分開一定的距離。<br>\n",
    "當角度低於較低的判斷閾值，才把狀態設置爲“蹲下”；角度高於較高的判斷閾值，才把狀態設置爲“起立”。<br>\n",
    "這樣的設計可以有效過濾二值化過程中，測量值在小範圍的波動引起的結果波動。<br>\n",
    "![img6-3.png](img6-3.png) <br>\n",
    "<br>\n",
    "```python\n",
    "if hip_angle<125 and knee_angle<105:\n",
    "    squat = 1\n",
    "elif hip_angle>145 and knee_angle>125:\n",
    "    if squat == 1:\n",
    "        repetition += 1\n",
    "    squat = 0\n",
    "```\n",
    "2. **Meaningless title** also determined the squatting status?<br>**毫無意義的片頭**也被用來判斷深蹲狀態？<br>\n",
    "Do not let the squat counter count when MoveNet is not confident with the result.<br>\n",
    "在MoveNet對結果不確定的時候，不要讓squat counter計數<br>\n",
    "<br>\n",
    "```python\n",
    "def .......(....):\n",
    "    ......\n",
    "    if (a1s>keypoint_threshold)*(b1s>keypoint_threshold)*(c1s>keypoint_threshold):\n",
    "        return (angle1 + angle2) / 2\n",
    "    else:\n",
    "        return None\n",
    "```\n",
    "```python\n",
    "if (hip_angle is not None) and (knee_angle is not None):\n",
    "    .........\n",
    "    text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "else:\n",
    "    text = f\"Reps: {repetition}  Knee: ?  Hip: ?\"\n",
    "```\n",
    "3. **Other** possible sources that trigger jittering?<br>**其他**可能觸發抖動的來源？<br>\n",
    "Collect multiple image samples and only update the squatting status when several consecutive results indicate squatting (standing).<br>\n",
    "Avoid random interference (such as background, casual movements) through multiple \"measurements\".<br>\n",
    "採集多個圖像樣本，只有連續幾張照片的結果都爲蹲下或站立才更新squat的狀態。<br>\n",
    "藉由多次“測量”避免偶然干擾（如背景、偶然動作）的影響。<br>\n",
    "<br>\n",
    "```python\n",
    "squatting = 0\n",
    "CONSECUTIVE_THRESHOLD = 3    # << You can change this!\n",
    "# squatting == 0: last consecutive 3 confident results from images are all standing\n",
    "# squatting == CONSECUTIVE_THRESHOLD: last consecutive 3 confident results from images are all squatting\n",
    "def frame_process(frame):\n",
    "    global repetition, squat, squatting\n",
    "    ..........\n",
    "    if hip_angle<125 and knee_angle<105:\n",
    "        # if MoveNet result is squatting\n",
    "        #   if current state is standing, squatting += 1\n",
    "        #   otherwise, reset squatting to CONSECUTIVE_TH\n",
    "        if squat == 0:\n",
    "            squatting += 1\n",
    "        else:\n",
    "            squatting = CONSECUTIVE_TH\n",
    "    elif hip_angle>145 and knee_angle>125:\n",
    "        # if MoveNet result is standing\n",
    "        #   if current state is squatting, squatting -= 1\n",
    "        #   otherwise, reset squatting to 0\n",
    "        if squat == 1:\n",
    "            squatting -= 1\n",
    "        else:\n",
    "            squatting = 0\n",
    "    if squatting == 0 and squat == 1:\n",
    "        repetition += 1\n",
    "        squat = 0\n",
    "    elif squatting == CONSECUTIVE_TH and squat == 0:\n",
    "        squat = 1\n",
    "    ...........\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be318ba6-0d01-4c1d-a979-6b848acb5ab5",
   "metadata": {},
   "source": [
    "### Final Version 最終版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e5746f-f07f-449e-9675-461938a28abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle\n",
    "\n",
    "def calculate_hip_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "    keypoints_with_scores = keypoints_with_scores[0][0]\n",
    "    \n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_shoulder\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_shoulder\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # if confident score of keypoints are all above threshold, return the midpoint of two angle\n",
    "    # otherwise, return None\n",
    "    if (a1s>keypoint_threshold)*(b1s>keypoint_threshold)*(c1s>keypoint_threshold):\n",
    "        return (angle1 + angle2) / 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_knee_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "    keypoints_with_scores = keypoints_with_scores[0][0]\n",
    "\n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_ankle\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_ankle\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # if confident score of keypoints are all above threshold, return the midpoint of two angle\n",
    "    # otherwise, return None\n",
    "    if (a1s>keypoint_threshold)*(b1s>keypoint_threshold)*(c1s>keypoint_threshold):\n",
    "        return (angle1 + angle2) / 2\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ba0e70-b27b-48a4-9898-9a0bea55b378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3204430bfe74c689dd4efabfb9a61b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:1] global ../modules/videoio/src/cap_gstreamer.cpp (961) open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def putText(frame, text, color = (0, 255, 0)):\n",
    "    # Define the text properties\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_position = (50, 50)\n",
    "    text_scale = 0.65\n",
    "    text_color = color\n",
    "    text_thickness = 2\n",
    "\n",
    "    # Add text annotation on the frame\n",
    "    cv2.putText(frame, text, text_position, font, text_scale, text_color, text_thickness)\n",
    "\n",
    "repetition = 0\n",
    "squat = 0\n",
    "squatting = 0\n",
    "CONSECUTIVE_TH = 3    # << You can change this!\n",
    "def frame_process(frame):\n",
    "    global repetition, squat, squatting\n",
    "    # calculate and display angle\n",
    "    keypoints_with_scores = get_keypoints_with_scores_from_image_with_movenet(frame,input_size,input_size)\n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n",
    "\n",
    "\n",
    "    # check if confidence is qualified, if yes, proceed to judgement\n",
    "    if hip_angle is not None and knee_angle is not None:\n",
    "        # if capture continuous CONSECUTIVE_TH angles in squatting, change squat to 1\n",
    "        # if capture continuous CONSECUTIVE_TH angles in standing, change squat to 0 and repetition += 1\n",
    "        if hip_angle<125 and knee_angle<105:\n",
    "            if squat == 0:\n",
    "                squatting += 1\n",
    "            else:\n",
    "                squatting = CONSECUTIVE_TH\n",
    "        elif hip_angle>145 and knee_angle>125:\n",
    "            if squat == 1:\n",
    "                squatting -= 1\n",
    "            else:\n",
    "                squatting = 0\n",
    "        if squatting == 0 and squat == 1:\n",
    "            repetition += 1\n",
    "            squat = 0\n",
    "        elif squatting == CONSECUTIVE_TH and squat == 0:\n",
    "            squat = 1\n",
    "        text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    else:\n",
    "        text = f\"Reps: {repetition}  Knee: ?  Hip: ?\"\n",
    "    if squat:\n",
    "        text_color = (0,255,0)\n",
    "    else:\n",
    "        text_color = (128,192,64)\n",
    "\n",
    "    # draw visualized prediction from MoveNet on the frame. Attention: Not recommended, VERY LAGGY!\n",
    "    #frame = draw_prediction_on_image( tf_image_to_model(frame,600,600)[0]/255, keypoints_with_scores)\n",
    "    \n",
    "    \n",
    "    putText(frame, text, text_color)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "# Run\n",
    "# ================\n",
    "if __name__ == \"__main__\":\n",
    "    display(stopButton)\n",
    "    thread = threading.Thread(target=view, args=(stopButton, 0, 0, frame_process))\n",
    "    thread.start()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
