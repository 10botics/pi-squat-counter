{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d9a3941-0e48-4dcb-93fd-f28598f63278",
   "metadata": {},
   "source": [
    "# Calculating Angles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae15aabb-9970-4c26-9d57-f9c129c66e9a",
   "metadata": {},
   "source": [
    "### Squat detection and counting 深蹲判斷與計數"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ab830b3-651b-4b74-8db9-28ce63715b95",
   "metadata": {},
   "source": [
    "In squat exercises, though **knee and hip joint angles** vary by individual ability, flexibility, and squatting techniques, these can be used as effective features to determine squatting down or standing up.<br>深蹲運動中，雖然膝蓋與臀部關節角度可能因個人體能、柔軟度和深蹲技巧而有所不同，但一般來說，**膝蓋與臀部關節的角度**可以作爲我們判別蹲下或起立的有效特徵。<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "148aa0a7-a716-4c5a-9900-f006897c4313",
   "metadata": {},
   "source": [
    "Above, we performed pose estimation on an image using MoveNet and defineed `get_keypoints_with_scores_from_image_with_movenet` to directly obtain MoveNet results from an image.<br>上面，我們可以通過MoveNet對圖像進行姿態辨識，並定義了`get_keypoints_with_scores_from_image_with_movenet`來直接從一張圖片獲取MoveNet結果<br>\n",
    "<br>\n",
    "How can we convert these results into angle?<br>\n",
    "我們如何把這些結果轉換成角度信息呢？<br>\n",
    "![img6-1](resource/img6-1.png)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c5a63e7-a24f-43ce-9032-d3b10643a65c",
   "metadata": {},
   "source": [
    "##### The `calculate_angle(a, b, c)` in the cell below obtains angle information using a such method.<br>下方單元格的`calculate_angle(a,b,c)`函數就是運用類似這樣的方法獲取角度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d2ff2e-54a4-4c10-b4fd-5abc15db9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f59f42-76d2-45e1-b3b5-c18acf1dbc4e",
   "metadata": {},
   "source": [
    "##### Let's use the `calculate_angle(a,b,c)` previously defined to create a function that calculates the angle between the knee and hip joints.<br>讓我們利用上方定義的calculate_angle(a,b,c)封裝計算膝關節與臀部角度的函數\n",
    "The input of the function is the keypoints detected by the MoveNet model with scores.<br>\n",
    "函數的輸入是MoveNet模型檢測到的帶有分數的關鍵點。<br>\n",
    "<br>\n",
    "`calculate_hip_joint_angle` calculates the angle of the left and right hip joints. It extracts positions of the hips, knees, and shoulders from keypoints, and calculates angles from the position. It finally returns the average of these two angles.<br>\n",
    "`calculate_hip_joint_angle`計算左右臀部的角度。它首先從關鍵點中提取左右臀部，膝蓋和肩膀的位置，然後從位置信息計算角度。最後返回這兩個角度的平均值。<br>\n",
    "<br>\n",
    "`calculate_hip_joint_angle` calculates the angle of the left and right knee joints in the same way<br>\n",
    "`calculate_hip_joint_angle` 函數用同樣的方法計算左右膝關節的角度。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b96f9a-03c5-4e55-a650-5347f199cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hip_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "    keypoints_with_scores = keypoints_with_scores[0][0]\n",
    "    \n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_shoulder\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_shoulder\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # return the midpoint of two angle\n",
    "    return (angle1 + angle2) / 2\n",
    "    \n",
    "\n",
    "def calculate_knee_joint_angle(keypoints_with_scores, keypoint_threshold = 0.2): \n",
    "    keypoints_with_scores = keypoints_with_scores[0][0]\n",
    "\n",
    "    # get the position of keypoints from MoveNet output\n",
    "    a1y, a1x, a1s = keypoints_with_scores[KEYPOINT_DICT[\"left_hip\"]]\n",
    "    a2y, a2x, a2s = keypoints_with_scores[KEYPOINT_DICT[\"right_hip\"]]\n",
    "    b1y, b1x, b1s = keypoints_with_scores[KEYPOINT_DICT[\"left_knee\"]]\n",
    "    b2y, b2x, b2s = keypoints_with_scores[KEYPOINT_DICT[\"right_knee\"]]\n",
    "    c1y, c1x, c1s = keypoints_with_scores[KEYPOINT_DICT[\"left_ankle\"]]\n",
    "    c2y, c2x, c2s = keypoints_with_scores[KEYPOINT_DICT[\"right_ankle\"]]\n",
    "\n",
    "    # calculate angle of left and right body respectively\n",
    "    angle1 = calculate_angle( (a1y, a1x), (b1y, b1x), (c1y, c1x) )\n",
    "    angle2 = calculate_angle( (a2y, a2x), (b2y, b2x), (c2y, c2x) )\n",
    "\n",
    "    # return the midpoint of two angle\n",
    "    return (angle1 + angle2) / 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca12a1d5-3aee-4420-9fa6-1dd42ea48338",
   "metadata": {},
   "source": [
    "##### Next, we will use OpenCV and functions above to detect body posture and calculate the number of squats.<br>接下來，我們利用OpenCV和上面的函數，檢測人的身體姿勢計算深蹲次數。\n",
    "<br>\n",
    "In each iteration, we pass the image frame to MoveNet and obtain the results, and then convert the results of MoveNet into angles of the knee joint and hip.<br>\n",
    "在每次循環中，我們將圖像幀傳給MoveNet並獲取姿態檢測的結果，將MoveNet的結果轉換爲膝關節和臀部的角度。<br>\n",
    "<br>\n",
    "When the angle is below a certain value, the `squat` is recorded as 1 (indicates squatted down). Otherwise, otherwise the `squat` is recorded as 0 (indicates standed up). We add 1 to the squat count when transitioning from squat to stand. (i.e., when squat==1 and angle is greater than the threshold ).<br>\n",
    "當角度小於一定值時，將`squat`記爲1（表示已蹲下）。否則，將`squat`記爲0（表示已起立）。我們將姿態從蹲下轉爲起立的瞬間（squat==1的同時角度大於判定值）將深蹲計數+1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d037dd-1886-485f-920a-2f340522fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "\n",
      "The following functions have been imported:\n",
      "   movenet(input_image)\n",
      "   _keypoints_and_edges_for_display(keypoints_with_scores, height, width, keypoint_threshold=0.11)\n",
      "   draw_prediction_on_image(image, keypoints_with_scores, crop_region=None, close_figure=False, output_image_height=None)\n",
      "The following global variables have been assigned:\n",
      "   module, model_name, input_size, KEYPOINT_DICT, KEYPOINT_EDGE_INDS_TO_COLOR\n",
      "=======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run Squat_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "166740f8-4d98-4887-8c95-c4f1bfbb7a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36c224967e4427da7d4208e4a9f6178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def putText(frame, text, color = (0, 255, 0)):\n",
    "    # Define the text properties\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_position = (50, 50)\n",
    "    text_scale = 0.65\n",
    "    text_color = color\n",
    "    text_thickness = 2\n",
    "\n",
    "    # Add text annotation on the frame\n",
    "    cv2.putText(frame, text, text_position, font, text_scale, text_color, text_thickness)\n",
    "\n",
    "repetition = 0\n",
    "squat = 0\n",
    "def frame_process(frame):\n",
    "    global repetition, squat\n",
    "    # calculate and display angle\n",
    "    keypoints_with_scores = get_keypoints_with_scores_from_image_with_movenet(frame,input_size,input_size)\n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n",
    "\n",
    "\n",
    "    # Gesture judgement from angle of hip-joint and knee-joint\n",
    "    # if gesture is from squat (squat == 1) change to stand (squat == 0), repetition add by 1\n",
    "    if hip_angle<135 and knee_angle<120:\n",
    "        squat = 1\n",
    "    elif hip_angle>135 and knee_angle>120:\n",
    "        if squat == 1:\n",
    "            repetition += 1\n",
    "        squat = 0\n",
    "    text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    if squat:\n",
    "        text_color = (0,255,0)\n",
    "    else:\n",
    "        text_color = (128, 192, 64)\n",
    "\n",
    "    # draw visualized prediction from MoveNet on the frame. Attention: Not recommended, VERY LAGGY!\n",
    "    #frame = draw_prediction_on_image( tf_image_to_model(frame,600,600)[0]/255, keypoints_with_scores)\n",
    "    \n",
    "    \n",
    "    putText(frame, text, text_color)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "# Run\n",
    "# ================\n",
    "if __name__ == \"__main__\":\n",
    "    display(stopButton)\n",
    "    thread = threading.Thread(target=view, args=(stopButton, \"Produce_2_direct.mp4\", 0, frame_process))\n",
    "    thread.start()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3646147f-e71a-4b2b-bc96-3fdb8c44f587",
   "metadata": {},
   "source": [
    "##### <br><hr>Evidently, the output results of MoveNet are mostly reliable, but sometimes there may be some jitter, which can cause counting errors! <br>\n",
    "What are the cause of these jitters?<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f593d4ff-ebb3-4f44-b76c-cbe542ff1731",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1. Output prone to shaking in **half-squat position**?<br>**半蹲狀態**的輸出容易抖動？\n",
    "2. **Meaningless title** also determined the squatting status?<br>**毫無意義的片頭**也被用來判斷深蹲狀態？<br>\n",
    "3. **Other** possible sources that trigger jittering?<br>**其他**可能觸發抖動的來源？<br>\n",
    "Collect multiple image samples and only update the squatting status when several consecutive results indicate squatting (standing).<br>\n",
    "Avoid random interference (such as background, casual movements) through multiple \"measurements\".<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
