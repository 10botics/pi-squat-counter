{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8513baff-cafb-4967-87cf-7f9f03072744",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"https://10botics.com/logo_jnb.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e130cd-45c6-4561-a3a4-ee596d311fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pose estimation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d79862-eecb-49cd-80d2-e8f92a298b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"./resource/estimation_on_image_sample.png\" width=300 />\n",
    "\n",
    "In this lesson, you will learn:\n",
    "\n",
    "1. Import the tensorflow libraries\n",
    "2. Download a pose detection model - movenet\n",
    "3. Load a sample image\n",
    "4. Resize the sample image\n",
    "5. Run the model\n",
    "6. Draw detected keypoints on the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2e056-ca58-479f-b568-33fa58e633ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac41de-8132-45e9-9353-ec838bbd988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Refer to install_tf.sh if you have not installed the tensorflow required libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b411281-d5fe-4200-a6aa-e0e7406894e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278ba95e-8b90-4190-969c-4a1886cd58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "print(\"TF version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205751d6-84b1-4ad0-92e6-2fbf89f75984",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a97fcd-7d6a-4b1a-99d2-73f11a98baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "We are going to use movenet as the sample. It is an ultra fast and accurate pose detection model. For more information about movenet, check below:\n",
    "\n",
    "- https://www.tensorflow.org/hub/tutorials/movenet\n",
    "- https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html\n",
    "- https://storage.googleapis.com/movenet/MoveNet.SinglePose%20Model%20Card.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b38d39c-fc80-4059-9b70-d0a68f3b1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model \n",
    "!wget -nc -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "\n",
    "# Initialize the TFLite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db96c70d-98d4-46f5-99de-adf4bc3d6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32d9082-ce7c-40ee-84e8-5888a6dab725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()    \n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8058f7e0-d91b-4047-ac54-a958d79b66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Input\n",
    "\n",
    "`interpreter.get_input_details()` returns the specification of the input the model requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31441e77-3590-431c-b484-38211f797e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1844f056-5ab0-44d3-8210-eeecc3930873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model input size\n",
    "model_input_size = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4264f-743a-4f68-a4e5-4825ccfbd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Output\n",
    "\n",
    "`interpreter.get_output_details()` returns the specification of the output the model generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f7181bc-89ea-4ab8-be02-7f8cf147251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b1d0d-f08f-4222-9381-bcd95637cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st Attempt: Feed the image to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21163555-e921-497e-875d-e7df515a9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc99567f-d99f-4cf3-8023-b2f5bb62a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the image\n",
    "image_path = 'resource/input_image4.jpeg'\n",
    "import matplotlib.image as mpimg\n",
    "image = mpimg.imread(image_path)\n",
    "imgplot = plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6207ef0f-ea0a-48a0-a7f0-fbad17f5696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line will generate an error! What's the problem?\n",
    "keypoints_with_scores = movenet(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f1312-b34d-4bb9-8813-028ede4de556",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Error\n",
    "\n",
    "- What is the meaning of this error message?\n",
    "- What is our image shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c69e5c7-55cd-463a-aa89-963656a32559",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227d61c-e689-4bf5-80a8-063e05805e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2nd Attempt: Adding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "264df88b-ac43-4665-b3b0-365abef19bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_image = tf.expand_dims(image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d658eb-118c-40de-984c-54480a40f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36fbb3b4-0f12-4f88-bdab-8c302d2a82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line will generate an error! What's the problem?\n",
    "\n",
    "keypoints_with_scores = movenet(expanded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b591949-2ae3-476a-8848-2e63355d9999",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3rd Attempt: Resize the image\n",
    "\n",
    "- Our model only accept a 192x192 image.\n",
    "- What is the size of our image?\n",
    "- How do we reduce it into 192x192?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07226619-3037-4beb-a1f9-ce193bbafc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = tf.image.resize_with_pad(expanded_image, model_input_size, model_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffbfc5-84bd-4685-b4d6-95f3e6977b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resized_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47750f3c-9953-416d-9b00-7b41b46251d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following lines will generate an error! What's the problem?\n",
    "\n",
    "imgplot = plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081e7a8-481a-49a4-962c-d2a1f80594c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise #1 - Fix the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c76a7bd-1bf0-407f-9f02-d1a495adb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9181cd-f09f-44f9-a51e-5694e9698a1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "824237aa-e8ec-41a7-8dbd-c9f55c9942ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(resized_image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f7103-f091-455e-b7f4-0da18471642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conform to imshow standard\n",
    "\n",
    "According to imshow documentation, the image data supported array shapes are:\n",
    "\n",
    "- (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n",
    "\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0572722-e355-4d37-ab20-92ae195b333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the left uppermost pixel\n",
    "resized_image[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae5b29-a884-4c31-99dc-c4b343291fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a pixel somewhere in the centre\n",
    "resized_image[0][100][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374897c-152f-47a6-99aa-05a32f832606",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resized_image[0][100][100].numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44054f-1636-486e-b174-cb339afbe97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5312ef23-2307-447a-9e06-48d1ff2ec2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(resized_image[0].numpy() / 255.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2774a68-c759-4c45-a1a3-7344b6c2a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b15f13e-308f-4c34-b55d-135d91796a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model inference.\n",
    "keypoints_with_scores = movenet(resized_image)\n",
    "print(keypoints_with_scores)\n",
    "keypoints_with_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45927d8c-66a0-4caa-a30b-44dc26193d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpret the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269de2e9-8105-4bdb-8d99-e7860a1e60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5229148-7de0-4da3-b6ff-18c6b950a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reorganize the points into a more readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a54fb380-6666-4dde-9823-adf40fda97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sides = [\"left\" , \"right\"]\n",
    "body_parts = [\"shoulder\", \"hip\", \"ankle\", \"knee\"]\n",
    "\n",
    "points = dict()\n",
    "\n",
    "for side in sides:\n",
    "    points[side] = dict()\n",
    "    for body_part in body_parts:\n",
    "        points[side][body_part] = keypoints_with_scores[0][0][KEYPOINT_DICT[f\"{side}_{body_part}\"]]\n",
    "\n",
    "\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ef31b633-228b-44a6-a36c-3b20ba966253",
   "metadata": {},
   "outputs": [],
   "source": [
    "points['left']['shoulder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86bf4e07-b013-40d6-997a-a6565a8296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "points['left']['knee']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d832d-0189-48ab-8df6-750e23b6cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Annotate the points on the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c006a12-f788-4263-bfa7-23547ee8cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw the annotation on the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc1af53f-95a9-4194-a217-fabada0abcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_and_show(image, points):\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    plt.imshow(image /255.0)\n",
    "    \n",
    "    for side in sides:\n",
    "        for body_part in body_parts:            \n",
    "            absolute_x = points[side][body_part][1] * width\n",
    "            absolute_y = points[side][body_part][0] * height\n",
    "            # print(f\"{side}_{body_part}, x = {absolute_x} , y = {absolute_y}\")\n",
    "            plt.plot(absolute_x, absolute_y, 'bo')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "annotate_and_show(image, points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b621579-bf9a-4ac5-bac7-2277fceb3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Draw the annotation on the resized and padded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbe3e690-ec75-4472-95dd-7f7553732c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_and_show(resized_image[0], points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf75d9-25a2-44fa-9b4c-d3dd041ecfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare the two output\n",
    "\n",
    "Which one is more accurate? Why?\n",
    "\n",
    "<img src=\"./resource/model_image_compare.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06b4e4-e045-4b6e-a4b6-fa728e54bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consolidate our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9dfa856-812c-4c10-a0ae-d7f20b0516c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    # Show the image\n",
    "\n",
    "    import matplotlib.image as mpimg\n",
    "    image = mpimg.imread(image_path)\n",
    "    \n",
    "    return image    \n",
    "\n",
    "def convert_to_movenet_format(image):\n",
    "    expanded_image = tf.expand_dims(image, axis=0)    \n",
    "    resized_image = tf.image.resize_with_pad(expanded_image, model_input_size, model_input_size)\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def reoganize_output(movenet_output):\n",
    "    sides = [\"left\" , \"right\"]\n",
    "    body_parts = [\"shoulder\", \"hip\", \"ankle\", \"knee\"]\n",
    "    \n",
    "    points = dict()\n",
    "    \n",
    "    for side in sides:\n",
    "        points[side] = dict()\n",
    "        for body_part in body_parts:\n",
    "            points[side][body_part] = movenet_output[0][0][KEYPOINT_DICT[f\"{side}_{body_part}\"]]\n",
    "\n",
    "    return points\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f4c7a-cf9e-4f84-9222-bedb009d1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's try different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd3d7d40-09b2-40a0-8140-9895d7450404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = 'resource/input_image.jpeg'\n",
    "image_path = 'resource/input_image2.jpeg'\n",
    "# image_path = 'resource/input_image3.jpeg'\n",
    "# image_path = 'resource/input_image4.jpeg'\n",
    "\n",
    "image = read_image(image_path)\n",
    "movenet_image = convert_to_movenet_format(image)\n",
    "movenet_output = movenet(movenet_image)\n",
    "points = reoganize_output(movenet_output)\n",
    "annotate_and_show(movenet_image[0], points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8bed0a-afb0-45c8-bdb6-6dd7c3943c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cccca4b-bbac-4aa2-9743-ad73c8337fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b376544-2517-4b95-913f-ce6f522ce5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Congratulation! You have finished this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a5467-8b99-4112-a85f-8c63a83d575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "This jupyter notebook is created by 10Botics. <br>\n",
    "For permission to use in school, please contact info@10botics.com <br>\n",
    "All rights reserved. 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
