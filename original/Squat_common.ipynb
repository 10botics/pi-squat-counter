{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3c86c5-37ea-4498-b558-7b575bb783ab",
   "metadata": {},
   "source": [
    "# Squat Common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96177708-f084-44c4-9c96-e03249515e2c",
   "metadata": {},
   "source": [
    "This files import libraries and contains common functions needed by the squat counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca540cc8-be1f-4ebd-bc3c-735a999c17ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 12:30:31.748930: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-20 12:30:31.929732: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-20 12:30:31.935785: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-05-20 12:30:31.935801: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-05-20 12:30:31.963853: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-20 12:30:32.653289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-20 12:30:32.653365: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-20 12:30:32.653371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# ===import the libraries===\n",
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import cv2 \n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7e76a3-c635-45e1-a89c-c28ad333c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Download the model \n",
    "!wget -nc -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "\n",
    "# Define model input size\n",
    "input_size = 192\n",
    "\n",
    "# Initialize the TFLite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfecbcb6-2f32-4595-8fba-77ecb2de8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cce3512-9a50-40e2-9c6d-9b045d737c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following functions have been imported:\n",
      "   movenet(input_image)\n",
      "   _keypoints_and_edges_for_display(keypoints_with_scores, height, width, keypoint_threshold=0.11)\n",
      "   draw_prediction_on_image(image, keypoints_with_scores, crop_region=None, close_figure=False, output_image_height=None)\n",
      "The following global variables have been assigned:\n",
      "   module, model_name, input_size, KEYPOINT_DICT, KEYPOINT_EDGE_INDS_TO_COLOR\n",
      "=======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # ===Download an image for pose detection (if no \"input_image.jpeg\" file in directory)===\n",
    "# import os \n",
    "# # Download the demo image, if doesn't exist in current folder\n",
    "# if \"input_image.jpeg\" not in os.listdir():\n",
    "#     os.system('curl -o input_image.jpeg https://images.pexels.com/photos/4384679/pexels-photo-4384679.jpeg --silent')\n",
    "# input_path = \"input_image.jpeg\"\n",
    "\n",
    "# print(\"Image loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ===Define dict KEYPOINT_EDGE_INDS_TO_COLOR===\n",
    "# ===Define _keypoints_and_edges_for_display===\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "    \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "    Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "    \n",
    "    Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the absolute coordinates of all keypoints of all detected entities;\n",
    "      * the absolute coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "    \"\"\"\n",
    "    keypoints_all = []\n",
    "    keypoint_edges_all = []\n",
    "    edge_colors = []\n",
    "    _, num_instances, _, _ = keypoints_with_scores.shape\n",
    "    \n",
    "    for idx in range(num_instances):\n",
    "        kpts_x = keypoints_with_scores[0, idx, :, 1]        # x coordinate, normalized\n",
    "        kpts_y = keypoints_with_scores[0, idx, :, 0]        # y coordinate, normalized\n",
    "        kpts_scores = keypoints_with_scores[0, idx, : , 2]  # score\n",
    "        kpts_absolute_xy = np.stack((width*kpts_x, height*kpts_y), axis=1)\n",
    "        kpts_above_thresh_absolute_xy = kpts_absolute_xy[\n",
    "            kpts_scores > keypoint_threshold, :]\n",
    "        keypoints_all.append(kpts_above_thresh_absolute_xy)\n",
    "\n",
    "        for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "            kpt_idx1 , kpt_idx2 = edge_pair[0], edge_pair[1]\n",
    "            if (kpts_scores[kpt_idx1] > keypoint_threshold and\n",
    "               kpts_scores[kpt_idx2] > keypoint_threshold):\n",
    "                x_start, y_start = kpts_absolute_xy[kpt_idx1]\n",
    "                x_end, y_end = kpts_absolute_xy[kpt_idx2]\n",
    "                line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "                keypoint_edges_all.append(line_seg)\n",
    "                edge_colors.append(color)\n",
    "    if keypoints_all:\n",
    "        keypoints_xy = np.concatenate(keypoints_all)\n",
    "    else:\n",
    "        keypoints_xy = np.zeros((0, 17, 2))\n",
    "    \n",
    "    if keypoint_edges_all:\n",
    "        edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "    else:\n",
    "        edges_xy = np.zeros((0, 2, 2))\n",
    "    return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "                                         \n",
    "# ===Define: draw_prediction_on_image===\n",
    "# changes: we will need to show too many pictures,\n",
    "#      so matplotlib is configured to \"Agg\" mode, and forced GC is used\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib import patches\n",
    "\n",
    "import gc\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "\n",
    "    \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "    Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "    with_padding: if the result keeps edge padding. Normally if the input image\n",
    "      to the MoveNet has padding but in display won't padding, with_padding = 0\n",
    "    \n",
    "    Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "    \"\"\"\n",
    "        \n",
    "    matplotlib.use(\"Agg\")     # important, memery leak will orrur if without this statement\n",
    "    \n",
    "    height, width, channel = image.shape\n",
    "    aspect_ratio = width / height\n",
    "\n",
    "    fig, ax = plt.subplots( figsize=( 12*aspect_ratio, 12 ) )\n",
    "    fig.tight_layout(pad = 0)\n",
    "    ax.margins(0)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    ax.imshow(image)\n",
    "    line_segments = LineCollection([], linewidths=(4), linestyle = 'solid' )\n",
    "    ax.add_collection(line_segments)\n",
    "\n",
    "    scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "    keypoint_locs, keypoint_edges, edge_colors = ( _keypoints_and_edges_for_display(\n",
    "        keypoints_with_scores, height, width) )\n",
    "        \n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "    if crop_region is not None:\n",
    "        xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "        ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "        rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "        rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin,ymin),rec_width,rec_height,\n",
    "            linewidth=1,edgecolor='b',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(\"all\")\n",
    "    plt.clf()\n",
    "    gc.collect()     # important, memery leak will orrur if without this statement\n",
    "    return image_from_plot\n",
    "\n",
    "\n",
    "        \n",
    "# ===Defined IPython widget: stopButton===\n",
    "from IPython.display import display, Image\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Description',\n",
    "    icon='square' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "\n",
    "print('''\n",
    "The following functions have been imported:\n",
    "   movenet(input_image)\n",
    "   _keypoints_and_edges_for_display(keypoints_with_scores, height, width, keypoint_threshold=0.11)\n",
    "   draw_prediction_on_image(image, keypoints_with_scores, crop_region=None, close_figure=False, output_image_height=None)\n",
    "The following global variables have been assigned:\n",
    "   module, model_name, input_size, KEYPOINT_DICT, KEYPOINT_EDGE_INDS_TO_COLOR\n",
    "=======================================\n",
    "'''\n",
    ")\n",
    "\n",
    "if __name__ != \"__main__\":\n",
    "    print(\"====== Importing from step 4 ... ======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28621c8a-c2ab-4887-b846-58a08d138292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_to_tf(image):\n",
    "    # convert image to tf format\n",
    "    # input argument \"image\" can be Array of image, or a string of the path to an image in jpeg format\n",
    "    if isinstance(image, str):\n",
    "        image = tf.io.read_file(image)\n",
    "        image_converted = tf.io.decode_jpeg(image)\n",
    "    else: \n",
    "        image_converted = tf.convert_to_tensor(image)\n",
    "    return image_converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b61d4ac-ef98-4bae-910e-15e59dc39b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_image_to_model(image, width, height):\n",
    "    # convert to model required format\n",
    "    image_expand_dim = tf.expand_dims(image, 0)\n",
    "    input_image = tf.image.resize_with_pad(image_expand_dim, width, height)\n",
    "    return input_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20be77de-5c0f-4d6c-8a81-5a800c3fdecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image):\n",
    "    print(image.shape)\n",
    "    # show an image using matplotlib. \n",
    "    # matplotlib is changed to \"Agg\" mode in the end to disable interaction to avoid memory leak\n",
    "    matplotlib.use('module://ipykernel.pylab.backend_inline')\n",
    "    plt.clf()\n",
    "    if isinstance(image, np.ndarray):\n",
    "        plt.imshow(image / 255)\n",
    "    else:\n",
    "        plt.imshow(image.numpy() / 255.0)\n",
    "    plt.show()\n",
    "    matplotlib.use('Agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63cbb6e-07b4-4a67-a1e1-eeef0760a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints_with_scores_from_image_with_movenet(image, width, height):\n",
    "    \"\"\"\n",
    "    Return the keypoints with scores from a given image using Movenet\n",
    "\n",
    "    Args:\n",
    "    image: can be Array of image, or a string of the path to the image\n",
    "    width: width of input image Model required\n",
    "    height: height of input image Model required\n",
    "\n",
    "    Return: \n",
    "    A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    \"\"\"\n",
    "    image_converted = load_image_to_tf(image)\n",
    "    input_image = tf_image_to_model(image_converted, width, height)\n",
    "    keypoints_with_scores = movenet(input_image)\n",
    "    return keypoints_with_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c0399b-e0e4-4eee-90b9-f5ac74c91c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display function\n",
    "# ================\n",
    "def view( button, source=0, rotate=0, process=(lambda x:x) ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    button: An IPywidget button to control the function. The display stops if button.value == True.\n",
    "    Source: An optional integer or filename to specify the source of the video stream.\n",
    "    Rotate: optional integer. Set to 0 by default (not rotated). Set to 1 will rotate by 90 deg.\n",
    "    process: optional function that processes each frame through process(frame) before displaying.\n",
    "        Set to identity function by default.\n",
    "    \"\"\"\n",
    "    display_handle=display(\"Please wait...\", display_id=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(source)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)\n",
    "\n",
    "    # print(\"something\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if button.value==True or not ret:\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n",
    "            button.value = False\n",
    "            break\n",
    "        # frame = cv2.flip(frame, 1) # if your camera reverses your image\n",
    "\n",
    "        # Rotate the frame if rotate==1\n",
    "        if rotate:\n",
    "            # frame = cv2.transpose(frame)\n",
    "            frame = cv2.flip(frame, 0)\n",
    "        \n",
    "        frame = process(frame)\n",
    "        \n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(Image(data=frame.tobytes()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
