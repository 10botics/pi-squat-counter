{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa236a0-7fe7-46f7-a433-ff75cd15d61a",
   "metadata": {},
   "source": [
    "# Applying model to a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6f8c5-1652-4df5-ba92-7d48f3ae0c5a",
   "metadata": {},
   "source": [
    "## Include libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e9227bc-f249-4da6-91ff-11719c68c8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 11:21:52.221779: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 11:21:52.429994: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-21 11:21:52.465387: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jonathantse/miniconda3/envs/squat/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-21 11:21:52.465401: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-05-21 11:21:52.491102: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 11:21:53.132945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jonathantse/miniconda3/envs/squat/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-21 11:21:53.133019: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jonathantse/miniconda3/envs/squat/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-21 11:21:53.133025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "%run Squat_common_v2.ipynb\n",
    "%run functions_angle_calculation.ipynb\n",
    "%run functions_video.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4dd8b-2460-4480-bf98-6c152b81be2b",
   "metadata": {},
   "source": [
    "## Playing a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77090d1f-bb28-4d09-bd75-9c11b2882bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this frame_process does nothing for now, just returning the frame\n",
    "\n",
    "def frame_process(frame):\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee0089e-7cfc-4d9b-abe3-d5e392259ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5e975d36d6453cae1f870ddac0513e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton, \"../resource/Produce_2_direct.mp4\", 0, frame_process))\n",
    "thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f859c-5aa8-4aac-aea2-6ee764dd0c4d",
   "metadata": {},
   "source": [
    "## Add some text to the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42c8ca7f-3ea6-4fe5-a232-b1db37700148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def putText(frame, text, color = (0, 255, 0), position_x = 50, position_y = 50):\n",
    "    # Define the text properties\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_position = (position_x, position_y)\n",
    "    text_scale = 0.65\n",
    "    text_color = color\n",
    "    text_thickness = 2\n",
    "\n",
    "    # Add text annotation on the frame\n",
    "    cv2.putText(frame, text, text_position, font, text_scale, text_color, text_thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a2a4e2b-907c-42e6-bb39-240cdf0dbcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5e975d36d6453cae1f870ddac0513e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def frame_process(frame):\n",
    "    text = f\"HAHAHA\"\n",
    "    putText(frame, text)\n",
    "    return frame\n",
    "\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton, \"../resource/Produce_2_direct.mp4\", 0, frame_process))\n",
    "thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b64737-9d3e-435b-a3e4-e20e10731e8a",
   "metadata": {},
   "source": [
    "### Add a counter and angles to the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "347126e0-9345-4dec-bcb8-8536555e5aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5e975d36d6453cae1f870ddac0513e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = 0\n",
    "knee_angle = 0\n",
    "hip_angle = 0\n",
    "\n",
    "def frame_process(frame):\n",
    "    global count\n",
    "    text = f\"Count: {count}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    count = count + 1\n",
    "    \n",
    "    putText(frame, text)\n",
    "    return frame\n",
    "\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton, \"../resource/Produce_2_direct.mp4\", 0, frame_process))\n",
    "thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847fccf-407e-441b-9a3d-79ed0eb8744d",
   "metadata": {},
   "source": [
    "## Calculate angle for each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13991d06-1178-4819-98e5-48b59fbf49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_process(frame):\n",
    "    global repetition, squat\n",
    "    # calculate and display angle\n",
    "    # keypoints_with_scores = get_keypoints_with_scores_from_image_with_movenet(frame,input_size,input_size)\n",
    "\n",
    "    movenet_frame = convert_to_movenet_format(frame)\n",
    "    movenet_output = movenet(movenet_frame)\n",
    "    keypoints_with_scores = reoganize_output(movenet_output)\n",
    "\n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0108ce8-0fe3-4520-b521-63d14a4ffcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_pad(image, target_height=None, target_width=None, pad_color=(0, 0, 0)):    \n",
    "    original_height, original_width = image.shape[:2]\n",
    "\n",
    "    if ((target_height is None) or (target_width is None)):\n",
    "        target_height = max(original_height, original_width)\n",
    "        target_width = target_height\n",
    "    \n",
    "    \n",
    "    # Calculate the scaling factor and new dimensions while maintaining aspect ratio\n",
    "    scale = min(target_width / original_width, target_height / original_height)\n",
    "    new_width = int(original_width * scale)\n",
    "    new_height = int(original_height * scale)\n",
    "    \n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Create a new image with the target dimensions and the padding color\n",
    "    padded_image = np.full((target_height, target_width, 3), pad_color, dtype=np.uint8)\n",
    "    \n",
    "    # Calculate the padding offsets\n",
    "    x_offset = (target_width - new_width) // 2\n",
    "    y_offset = (target_height - new_height) // 2\n",
    "    \n",
    "    # Place the resized image in the center of the padded image\n",
    "    padded_image[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_image\n",
    "    \n",
    "    return padded_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee4ab67b-cbfc-4de3-b910-6d607daf7c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5e975d36d6453cae1f870ddac0513e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repetition = 0\n",
    "squat = 0\n",
    "\n",
    "def absolute_xy(frame, point):\n",
    "    x = point[1] * frame.shape[1]\n",
    "    y = point[0] * frame.shape[0]\n",
    "    return (int(x), int(y))\n",
    "    \n",
    "def frame_process(frame):\n",
    "    global repetition, squat\n",
    "    # calculate and display angle\n",
    "    # keypoints_with_scores = get_keypoints_with_scores_from_image_with_movenet(frame,input_size,input_size)\n",
    "\n",
    "    movenet_frame = convert_to_movenet_format(frame)\n",
    "    movenet_output = movenet(movenet_frame)\n",
    "    keypoints_with_scores = reoganize_output(movenet_output)\n",
    "\n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n",
    "\n",
    "    frame = resize_with_pad(frame)\n",
    "    # frame = movenet_frame.numpy()\n",
    "    # print(type(movenet_frame[0]))\n",
    "    # print(movenet_frame[0])\n",
    "    # frame = movenet_frame[0].numpy()\n",
    "\n",
    "\n",
    "    # Gesture judgement from angle of hip-joint and knee-joint\n",
    "    # if gesture is from squat (squat == 1) change to stand (squat == 0), repetition add by 1\n",
    "    if hip_angle<135 and knee_angle<120:\n",
    "        squat = 1\n",
    "    elif hip_angle>135 and knee_angle>120:\n",
    "        if squat == 1:\n",
    "            repetition += 1\n",
    "        squat = 0\n",
    "    text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    if squat:\n",
    "        text_color = (0,255,0)\n",
    "    else:\n",
    "        text_color = (128, 192, 64)\n",
    "\n",
    "    putText(frame, text, text_color)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "# Run\n",
    "# ================\n",
    "\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton, \"../resource/Produce_2_direct.mp4\", 0, frame_process, 10))\n",
    "thread.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc77e4-93ea-44bf-addb-df8785f0841b",
   "metadata": {},
   "source": [
    "## Annotate the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ac9b2eb-9757-40ff-bfe0-78bdb741b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the color (BGR) and size\n",
    "\n",
    "def cv2_circle(frame, xy, text=None):\n",
    "    \n",
    "    color = (255, 0, 0)  # Blue color\n",
    "    radius = 5\n",
    "    thickness = -1  # Solid circle\n",
    "    \n",
    "    # Draw the point\n",
    "    cv2.circle(frame, xy, radius, color, thickness)\n",
    "\n",
    "    if text is not None:\n",
    "        x,y = xy\n",
    "        \n",
    "        putText(frame, text, (255,0,0),  int(x * 1.1), y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed35b7-94f7-4c4d-a8fd-ac2ad4e4d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "repetition = 0\n",
    "squat = 0\n",
    "\n",
    "def frame_process(frame):\n",
    "    global repetition, squat\n",
    "    # calculate and display angle\n",
    "    # keypoints_with_scores = get_keypoints_with_scores_from_image_with_movenet(frame,input_size,input_size)\n",
    "\n",
    "    movenet_frame = convert_to_movenet_format(frame)\n",
    "    movenet_output = movenet(movenet_frame)\n",
    "    keypoints_with_scores = reoganize_output(movenet_output)\n",
    "\n",
    "    hip_angle = calculate_hip_joint_angle(keypoints_with_scores)\n",
    "    knee_angle = calculate_knee_joint_angle(keypoints_with_scores)\n",
    "\n",
    "    frame = resize_with_pad(frame)\n",
    "    # frame = movenet_frame.numpy()\n",
    "    # print(type(movenet_frame[0]))\n",
    "    # print(movenet_frame[0])\n",
    "    # frame = movenet_frame[0].numpy()\n",
    "\n",
    "\n",
    "    # Gesture judgement from angle of hip-joint and knee-joint\n",
    "    # if gesture is from squat (squat == 1) change to stand (squat == 0), repetition add by 1\n",
    "    if hip_angle<135 and knee_angle<120:\n",
    "        squat = 1\n",
    "    elif hip_angle>135 and knee_angle>120:\n",
    "        if squat == 1:\n",
    "            repetition += 1\n",
    "        squat = 0\n",
    "    text = f\"Reps: {repetition}  Knee: {int(knee_angle)}  Hip: {int(hip_angle)}\"\n",
    "    if squat:\n",
    "        text_color = (0,255,0)\n",
    "    else:\n",
    "        text_color = (128, 192, 64)\n",
    "\n",
    "    # # draw visualized prediction from MoveNet on the frame. Attention: Not recommended, VERY LAGGY!\n",
    "    # #frame = draw_prediction_on_image( tf_image_to_model(frame,600,600)[0]/255, keypoints_with_scores)\n",
    "    \n",
    "    \n",
    "    putText(frame, text, text_color)\n",
    "    # hip_x, hip_y = \n",
    "    # hip_x, hip_y = absolute_xy(frame, keypoints_with_scores['right']['shoulder'])\n",
    "\n",
    "    for body_part in body_parts:    \n",
    "        cv2_circle(frame, absolute_xy(frame, keypoints_with_scores['right'][body_part]), body_part)\n",
    "    \n",
    "    # cv2_circle(frame, absolute_xy(frame, keypoints_with_scores['right']['shoulder']))\n",
    "    # cv2_circle(frame, absolute_xy(frame, keypoints_with_scores['right']['knee']))\n",
    "    # cv2_circle(frame, absolute_xy(frame, keypoints_with_scores['right']['ankle']))\n",
    "    \n",
    "    \n",
    "    putText(frame, \"hip\", text_color)\n",
    "    \n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "# Run\n",
    "# ================\n",
    "\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton, \"../resource/Produce_2_direct.mp4\", 0, frame_process, 10))\n",
    "thread.start()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
